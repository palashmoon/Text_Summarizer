{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here in text-summarizaton we will be using 2 technique to solve this problem on any wikipedia page or from any website.\n",
    "#1. Text Rank\n",
    "#2. Sentence embedding(Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#references\n",
    "# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will give a url of a wiki page and extract \n",
    "\n",
    "def extract_text(url):\n",
    "    #1.this will ping the website and return you html of a website.\n",
    "    website_url  = requests.get(url).text\n",
    "\n",
    "    #2. beautiful Soup is python package used for parsing html and xml document.\n",
    "    #it create a parse tree that is use to extract data from the html page\n",
    "    soup = BeautifulSoup(website_url , 'lxml')\n",
    "    \n",
    "    #pretiffy is use to show how the tags are used inside the html file.\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    #3. extract all p tag which content text\n",
    "    text = soup.find_all('p')\n",
    "    content_text = []\n",
    "    for text in text:\n",
    "        content_text.append(text.getText())\n",
    "        \n",
    "    return content_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    # 1.convert list to string\n",
    "    # 2.remove \\n from the string\n",
    "    # 3.remove square brackets\n",
    "    # 4.remove every thing except alphabets\n",
    "    # 5.remove extra spaces\n",
    "    list_to_string = ' '.join([str(i) for i in text]) \n",
    "    clean_data = re.sub('\\n' , \"\" , list_to_string)\n",
    "    clean_data = re.sub(r'\\[[0-9]*\\]', \" \" , clean_data)\n",
    "    clean_data = re.sub(r'[^a-zA-Z.]' , \" \" , clean_data)\n",
    "    clean_data = re.sub('\\s+' , \" \" , clean_data)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_preprocessing(clean_data):\n",
    "    # 1. tokenize into sentences\n",
    "    # 2. lower case\n",
    "    # 3. remove stopword\n",
    "    \n",
    "    sentence_token = sent_tokenize(clean_data)\n",
    "    lower_case = [i.lower() for i in sentence_token]\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    preprocess_data = \" \".join([i for i in lower_case if i not in stop_words])\n",
    "    return preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(filename):\n",
    "    word_embeddings = {}\n",
    "    f = open(filename + \".txt\",'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(text_preprocessing , model):\n",
    "    sentence_vector = []\n",
    "    for i in text_preprocessing:\n",
    "        if len(i) != 0:\n",
    "            v = sum([model.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "             v = np.zeros((100,))\n",
    "        sentence_vector.append(v)\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_mat():\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                  sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    text = extract_text('https://en.wikipedia.org/wiki/Taj_Mahal')\n",
    "    clean_data = clean_data(text)\n",
    "    text_preprocessing = text_preprocessing(clean_data)\n",
    "    model = loadGloveModel('../glove.6B.100d')\n",
    "    sentence_vector = sentence_vector(text_preprocessing , model)\n",
    "    similarity_matrix = similarity_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
